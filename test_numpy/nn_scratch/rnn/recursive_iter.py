# blog:http://www.cs.toronto.edu/~rgrosse/csc321/lec10.pdf
import numpy as np
import matplotlib.pyplot as plt
import math

# 迭代的
"""
We can determine the behavior of repeated iterations visually:
The behavior of the system can be summarized with a phase plot:
"""
def f1():
    # x(t+1) = xt^2+0.15
    # xt在（0,1）之间
    n= 30
    #xt= 0.8
    #xt= 0.1
    xt= 0.81  # 0.18377
    xt= 0.812  # 0.18377
    xt= 0.819  # out of range
    xt_list = []
    for i in range(n):
        xt = xt**2+0.15
        xt_list.append(xt)
    print("xt list:", "\n".join(["%d %.5f"%(i,x) for (i,x) in enumerate(xt_list)]))
    plt.plot(xt_list)
    plt.show()
    # xt list: [0.15, 0.1725, 0.17975624999999998, 0.18231230941406248, 0.18323777816388886, 0.18357608334643855, 0.18370017837681854, 0.18374575553567496, 0.183762502677376, 0.1837686573902526, 0.18377091943901602, 0.1837717508314613, 0.18377205640366068, 0.18377216871483024, 0.18377220999415203, 0.1837722251661347, 0.1837722307425125, 0.18377223279207924, 0.18377223354538616, 0.18377223382225993, 0.18377223392402336, 0.18377223396142595, 0.18377223397517306, 0.18377223398022574, 0.18377223398208284, 0.1837722339827654, 0.1837722339830163, 0.1837722339831085, 0.18377223398314235, 0.18377223398315481, 0.1837722339831594, 0.1837722339831611, 0.1837722339831617, 0.18377223398316192, 0.183772233983162, 0.18377223398316203, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206, 0.18377223398316206]
    # 不管x的初始值是多少，最后收敛到0.1837

f1()

"""
Now consider an RNN with a single logistic hidden
unit with weight 6 and bias -3, and no inputs or
outputs. This corresponds to the iteration

ht = f (ht−1) = σ (6ht−1 − 3).

This function is shown on the right. Let r = h1
denote the initial activation. The fixed points are
h = 0.07, 0.5, 0.93.

The gradient explodes for r = 0.5 and vanishes elsewhere.

Some observations:
Fixed points of f correspond to points where f crosses the line xt+1 = xt
.
Fixed points with f
0
(xt) > 1 correspond to sources.
Fixed points with f
0
(xt) < 1 correspond to sinks.
"""
def sigmoid(x):
    return 1/(1+math.exp(-x))

def f2():
    xt = 0
    xt = 0.49
    xt = 0.94
    n = 30
    w=6
    bias=-3
    xt_list=[]
    for i in range(n):
        logit = w*xt+bias
        xt = sigmoid(logit)
        xt_list.append(xt)
    print("xt list:", "\n".join(["%d %.5f"%(i,x) for (i,x) in enumerate(xt_list)]))
    plt.plot(xt_list)
    plt.show()
    #  # 不管x的初始值是多少，最后收敛到0.07,0.5,0.92928

#f2()